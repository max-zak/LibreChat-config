version: 1.2.8
cache: true

# Custom interface configuration
interface:
  # MCP Servers UI configuration
  mcpServers:
    placeholder: 'MCP Servers'

endpoints:
  custom:
# GitHub 
    # Models: https://models.inference.ai.azure.com/models
    - name: "Github Models"
      iconURL: https://github.githubassets.com/assets/GitHub-Mark-ea2971cee799.png
      apiKey:  '${GITHUB_TOKEN}'
      baseURL: "https://models.github.ai/inference"
      models:
        default:
          - openai/gpt-4.1
          - openai/gpt-4.1-mini
          - Meta-Llama-3-70B-Instruct
          - Meta-Llama-3-8B-Instruct
          - Meta-Llama-3.1-405B-Instruct
          - Meta-Llama-3.1-70B-Instruct
          - Meta-Llama-3.1-8B-Instruct
          - Mistral-Nemo
          - Mistral-large-2407
          - Mistral-small
          - gpt-4o
          - gpt-4o-mini

          - text-embedding-3-large
          - text-embedding-3-small
        fetch: false
      titleConvo: true
      titleModel: "gpt-4o-mini"

    # OpenRouter Example
    - name: 'OpenRouter'
      # For `apiKey` and `baseURL`, you can use environment variables that you define.
      # recommended environment variables:
      # Known issue: you should not use `OPENROUTER_API_KEY` as it will then override the `openAI` endpoint to use OpenRouter as well.
      apiKey: '${OPENROUTER_KEY}'
      baseURL: 'https://openrouter.ai/api/v1'
      models:
        default: ['meta-llama/llama-3-70b-instruct']
        fetch: true
      titleConvo: true
      titleModel: 'meta-llama/llama-3-70b-instruct'
      # Recommended: Drop the stop parameter from the request as Openrouter models use a variety of stop tokens.
      dropParams: ['stop']
      modelDisplayLabel: 'OpenRouter'