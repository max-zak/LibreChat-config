version: 1.2.8
cache: true

# Custom interface configuration
interface:
  # MCP Servers UI configuration
  mcpServers:
    placeholder: 'MCP Servers'

endpoints:
  custom:
# GitHub 
    # Models: https://models.inference.ai.azure.com/models
    - name: "Github Models"
      iconURL: https://github.githubassets.com/assets/GitHub-Mark-ea2971cee799.png
      apiKey:  '${GITHUB_TOKEN}'
      baseURL: "https://models.github.ai/inference"
      models:
        default:
          - gpt-4.1
          - gpt-4.1-mini
          - gpt-4.1-nano
          - gpt-4o
          - gpt-4o-mini
          - DeepSeek-R1-0528
          - Llama-4-Maverick-17B-128E-Instruct-FP8
          - Llama-4-Scout-17B-16E-Instruct
          - Llama-3.3-70B-Instruct
          - grok-3
          - Mistral-Nemo
          - Mistral-large-2411
          - mistral-medium-2505
          - mistral-small-2503
        fetch: false
      titleConvo: true
      titleModel: "gpt-4o-mini"

# OpenRouter Example
    - name: 'OpenRouter'
      # For `apiKey` and `baseURL`, you can use environment variables that you define.
      # recommended environment variables:
      # Known issue: you should not use `OPENROUTER_API_KEY` as it will then override the `openAI` endpoint to use OpenRouter as well.
      apiKey: '${OPENROUTER_KEY}'
      baseURL: 'https://openrouter.ai/api/v1'
      models:
        default: ['meta-llama/llama-3-70b-instruct']
        fetch: true
      titleConvo: true
      titleModel: 'meta-llama/llama-3-70b-instruct'
      # Recommended: Drop the stop parameter from the request as Openrouter models use a variety of stop tokens.
      dropParams: ['stop']
      modelDisplayLabel: 'OpenRouter'